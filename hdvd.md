# HÆ¯á»šNG DáºªN Váº¤N ÄÃP - Dá»° ÃN MULTIMODAL SEARCH

## á»¨NG Dá»¤NG ELASTICSEARCH XÃ‚Y Dá»°NG Há»† THá»NG TÃŒM KIáº¾M ÄA PHÆ¯Æ NG TIá»†N

---

## ğŸ“‹ Má»¤C Lá»¤C

1. [Tá»•ng quan dá»± Ã¡n](#1-tá»•ng-quan-dá»±-Ã¡n)
2. [Luá»“ng hoáº¡t Ä‘á»™ng há»‡ thá»‘ng](#2-luá»“ng-hoáº¡t-Ä‘á»™ng-há»‡-thá»‘ng)
3. [CÃ¢u há»i vá» Ä‘áº·t váº¥n Ä‘á» & má»¥c tiÃªu](#3-cÃ¢u-há»i-vá»-Ä‘áº·t-váº¥n-Ä‘á»--má»¥c-tiÃªu)
4. [CÃ¢u há»i vá» cÃ´ng nghá»‡](#4-cÃ¢u-há»i-vá»-cÃ´ng-nghá»‡)
5. [CÃ¢u há»i vá» dataset](#5-cÃ¢u-há»i-vá»-dataset)
6. [CÃ¢u há»i vá» kiáº¿n trÃºc há»‡ thá»‘ng](#6-cÃ¢u-há»i-vá»-kiáº¿n-trÃºc-há»‡-thá»‘ng)
7. [CÃ¢u há»i vá» triá»ƒn khai](#7-cÃ¢u-há»i-vá»-triá»ƒn-khai)
8. [CÃ¢u há»i vá» káº¿t quáº£](#8-cÃ¢u-há»i-vá»-káº¿t-quáº£)
9. [CÃ¢u há»i khÃ³ & cÃ¡ch tráº£ lá»i](#9-cÃ¢u-há»i-khÃ³--cÃ¡ch-tráº£-lá»i)
10. [Demo tips](#10-demo-tips)
11. [Checklist chuáº©n bá»‹](#11-checklist-chuáº©n-bá»‹)

---

## 1. Tá»”NG QUAN Dá»° ÃN

### ğŸ¯ **Má»¥c tiÃªu chÃ­nh:**

XÃ¢y dá»±ng há»‡ thá»‘ng tÃ¬m kiáº¿m thÃ´ng minh cho dá»¯ liá»‡u Ä‘a phÆ°Æ¡ng tiá»‡n (images, videos, audios) sá»­ dá»¥ng AI embeddings vÃ  Elasticsearch cluster phÃ¢n tÃ¡n.

### ğŸ”‘ **Äiá»ƒm nháº¥n:**

- **Big Data:** Xá»­ lÃ½ 900MB multimedia data, sáºµn sÃ ng scale lÃªn GB-TB
- **AI Integration:** CLIP model cho semantic & cross-modal search
- **Distributed System:** Elasticsearch cluster 3 nodes, auto sharding & replication
- **Real-world Application:** 10 real HD videos tá»« Pexels, khÃ´ng pháº£i demo Ä‘Æ¡n giáº£n

### ğŸ“Š **Sá»‘ liá»‡u quan trá»ng (ghi nhá»›):**

- Dataset: **1,010 files**, **911.88 MB**
- Cluster: **3 nodes** (es01, es02, es03)
- Sharding: **3 primary + 3 replica = 6 shards**
- Performance: **74ms** latency, **13.4 QPS**
- Embedding: **512-dimensional vectors**
- Model: **CLIP ViT-B/32** (400M parameters pre-trained)

---

## 2. LUá»’NG HOáº T Äá»˜NG Há»† THá»NG

### ğŸ”„ **Tá»”NG QUAN LUá»’NG HOáº T Äá»˜NG**

Há»‡ thá»‘ng hoáº¡t Ä‘á»™ng theo 2 giai Ä‘oáº¡n chÃ­nh: **Offline (Setup)** vÃ  **Online (Search)**

---

### ğŸ“¥ **GIAI ÄOáº N 1: OFFLINE - CHUáº¨N Bá»Š Dá»® LIá»†U (Cháº¡y 1 láº§n)**

#### **BÆ°á»›c 1.1: Thu tháº­p dá»¯ liá»‡u raw**
```
INPUT: 
- Pexels API â†’ 10 HD videos (738MB)
- PIL script â†’ 800 synthetic images (55MB)
- scipy script â†’ 200 synthetic audios (118MB)

OUTPUT:
data/raw/
â”œâ”€â”€ images/       (800 files, 55MB)
â”œâ”€â”€ pexels_videos/ (10 files, 738MB)
â””â”€â”€ audios/       (200 files, 118MB)
TOTAL: 1,010 files, 911.88MB
```

#### **BÆ°á»›c 1.2: Táº¡o embeddings tá»« raw data**
```python
# File: 3_create_embeddings.py

# 1. Load CLIP model
model, preprocess = clip.load("ViT-B/32", device="cpu")

# 2. Process IMAGES
for image_file in images:
    image = Image.open(image_file)           # Load
    image_tensor = preprocess(image)         # Resize 224x224, normalize
    embedding = model.encode_image(image_tensor)  # â†’ 512-d vector
    save_to_npy(embedding)                   # Save

# 3. Process VIDEOS (keyframe extraction)
for video_file in videos:
    cap = cv2.VideoCapture(video_file)
    keyframes = []
    while cap.isOpened():
        ret, frame = cap.read()
        if frame_count % 30 == 0:           # 1 frame/second
            frame_tensor = preprocess(frame)
            embedding = model.encode_image(frame_tensor)
            keyframes.append(embedding)
    
    video_embedding = np.mean(keyframes, axis=0)  # Average pooling
    save_to_npy(video_embedding)

# 4. Process AUDIOS (spectrogram approach)
for audio_file in audios:
    y, sr = librosa.load(audio_file)
    spectrogram = librosa.feature.melspectrogram(y, sr)
    spec_image = convert_to_image(spectrogram)  # Convert to PIL Image
    embedding = model.encode_image(spec_image)  # â†’ 512-d vector
    save_to_npy(embedding)
```

**OUTPUT:**
```
data/embeddings/
â”œâ”€â”€ images_embeddings.npy     (800 Ã— 512 floats)
â”œâ”€â”€ videos_embeddings.npy     (10 Ã— 512 floats)
â””â”€â”€ audios_embeddings.npy     (200 Ã— 512 floats)
TOTAL: ~2MB vectors
```

#### **BÆ°á»›c 1.3: Start Elasticsearch Cluster**
```bash
# File: docker-compose-cluster.yml
docker-compose -f docker-compose-cluster.yml up -d

# Containers started:
# - es01 (Master + Data node, port 9200)
# - es02 (Data node, port 9201)
# - es03 (Data node, port 9202)
# - kibana (Monitoring, port 5601)

# Wait ~30s for cluster formation
# Check status: curl http://localhost:9200/_cluster/health
# Expected: {"status": "green", "number_of_nodes": 3}
```

#### **BÆ°á»›c 1.4: Create Index vá»›i Vector Mapping**
```python
# File: 4_elasticsearch_index.py

# 1. Define mapping
mapping = {
    "mappings": {
        "properties": {
            "filename": {"type": "keyword"},
            "media_type": {"type": "keyword"},  # image/video/audio
            "embedding": {
                "type": "dense_vector",
                "dims": 512,
                "index": True,
                "similarity": "cosine"      # Cosine similarity for search
            },
            "file_path": {"type": "text"},
            "file_size": {"type": "long"}
        }
    },
    "settings": {
        "number_of_shards": 3,      # Split data into 3 shards
        "number_of_replicas": 1     # 1 backup copy each shard
    }
}

# 2. Create index
es.indices.create(index="multimedia", body=mapping)
```

#### **BÆ°á»›c 1.5: Bulk Index Documents**
```python
# Load embeddings
images_emb = np.load("data/embeddings/images_embeddings.npy")
videos_emb = np.load("data/embeddings/videos_embeddings.npy")
audios_emb = np.load("data/embeddings/audios_embeddings.npy")

# Prepare bulk data
actions = []
for i, emb in enumerate(images_emb):
    actions.append({
        "_index": "multimedia",
        "_source": {
            "filename": f"image_{i:04d}.jpg",
            "media_type": "image",
            "embedding": emb.tolist(),  # Convert numpy to list
            "file_path": f"data/raw/images/image_{i:04d}.jpg",
            "file_size": get_file_size(...)
        }
    })

# Similar for videos and audios...

# Bulk index
from elasticsearch.helpers import bulk
bulk(es, actions)

# Result: 1,010 documents indexed
# Distributed across 3 primary shards:
#   - Shard 0: ~337 docs
#   - Shard 1: ~337 docs  
#   - Shard 2: ~336 docs
# Each shard has 1 replica â†’ Total 6 shards
```

**OFFLINE PHASE COMPLETE!** âœ…
- Data stored: ES cluster (6 shards across 3 nodes)
- Ready for search queries

---

### ğŸ” **GIAI ÄOáº N 2: ONLINE - TÃŒM KIáº¾M (Cháº¡y má»—i query)**

#### **Workflow khi user search:**

```
USER INPUT
    â†“
[1] QUERY ENCODING (AI Processing)
    â†“
[2] ELASTICSEARCH VECTOR SEARCH (Distributed)
    â†“
[3] RANKING & RETURN RESULTS
    â†“
USER OUTPUT
```

---

#### **[1] QUERY ENCODING - Chuyá»ƒn query thÃ nh vector**

**Case A: Text Query**
```python
# User: "a person playing guitar"
query_text = "a person playing guitar"

# Encode text â†’ 512-d vector
text_tokens = clip.tokenize([query_text])
query_vector = model.encode_text(text_tokens)
# Result: [0.123, -0.456, 0.789, ..., 0.234]  (512 numbers)
```

**Case B: Image Query**
```python
# User uploads: ocean_photo.jpg
query_image = Image.open("ocean_photo.jpg")

# Encode image â†’ 512-d vector
image_tensor = preprocess(query_image)
query_vector = model.encode_image(image_tensor)
# Result: [0.234, -0.567, 0.123, ..., 0.456]  (512 numbers)
```

**KEY POINT:** 
- Text vÃ  Image Ä‘á»u â†’ **cÃ¹ng 512-d vector space**
- â†’ CÃ³ thá»ƒ compare trá»±c tiáº¿p!
- â†’ Cross-modal search works!

---

#### **[2] ELASTICSEARCH VECTOR SEARCH - TÃ¬m tÆ°Æ¡ng tá»±**

```python
# Build KNN (K-Nearest Neighbors) search query
search_query = {
    "knn": {
        "field": "embedding",              # Search in embedding field
        "query_vector": query_vector.tolist(),  # Our 512-d vector
        "k": 10,                           # Return top 10 results
        "num_candidates": 100              # Consider 100 candidates (HNSW)
    },
    "_source": ["filename", "media_type", "file_path"]
}

# Execute search
response = es.search(index="multimedia", body=search_query)
```

**What happens inside Elasticsearch:**

```
STEP 1: Query Distribution (Coordinator Node - es01)
    â”œâ”€> Send query to Shard 0 (on es01)
    â”œâ”€> Send query to Shard 1 (on es02)
    â””â”€> Send query to Shard 2 (on es03)

STEP 2: Parallel Search on Each Shard (HNSW Algorithm)
    
    Shard 0 (337 docs):
    â”œâ”€ Calculate cosine similarity for each doc:
    â”‚  similarity = dot(query_vector, doc_embedding) / (norm(query) * norm(doc))
    â”œâ”€ Use HNSW graph: O(log N) instead of O(N)
    â””â”€ Return top 10: [doc_5: 0.92, doc_123: 0.89, ...]
    
    Shard 1 (337 docs):
    â””â”€ Return top 10: [doc_456: 0.94, doc_234: 0.87, ...]
    
    Shard 2 (336 docs):
    â””â”€ Return top 10: [doc_789: 0.91, doc_567: 0.88, ...]

STEP 3: Merge Results (Coordinator Node)
    â”œâ”€ Collect 30 results (10 from each shard)
    â”œâ”€ Sort by similarity score: 0.94 > 0.92 > 0.91 > ...
    â””â”€ Return global top 10

Total time: ~74ms
    â”œâ”€ Network: 5ms
    â”œâ”€ Each shard search: 60ms (parallel)
    â”œâ”€ Merge: 7ms
    â””â”€ Response: 2ms
```

---

#### **[3] RANKING & RETURN RESULTS**

```python
# Parse response
results = []
for hit in response['hits']['hits']:
    results.append({
        'filename': hit['_source']['filename'],
        'media_type': hit['_source']['media_type'],
        'similarity': hit['_score'],          # Cosine similarity score
        'file_path': hit['_source']['file_path']
    })

# Display to user
print("ğŸ” Search Results:")
for i, result in enumerate(results, 1):
    stars = "â­" * int(result['similarity'] * 5)
    print(f"{i}. {result['filename']} ({result['media_type']}) - {stars}")
    print(f"   Similarity: {result['similarity']:.3f}")
```

**Example Output:**
```
ğŸ” Search Results for: "a person playing guitar"

1. pexels_1192116.mp4 (video) - â­â­â­â­
   Similarity: 0.823
   
2. image_0432.jpg (image) - â­â­â­â­
   Similarity: 0.791
   
3. audio_142.wav (audio) - â­â­â­â­
   Similarity: 0.774
   
4. image_0788.jpg (image) - â­â­â­
   Similarity: 0.742
   
5. pexels_2468101.mp4 (video) - â­â­â­
   Similarity: 0.718
```

---

### ğŸ¯ **LUá»’NG Äáº¦Y Äá»¦ - VÃ Dá»¤ THá»°C Táº¾**

**Scenario: User tÃ¬m "ocean waves"**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER: python demo_multimodal_search.py                      â”‚
â”‚ > Enter search query: ocean waves                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: ENCODE QUERY (Client Side)                          â”‚
â”‚ â”œâ”€ Load CLIP model                                          â”‚
â”‚ â”œâ”€ Tokenize "ocean waves"                                   â”‚
â”‚ â”œâ”€ model.encode_text() â†’ [0.234, -0.123, ..., 0.456]       â”‚
â”‚ â””â”€ Time: 15ms                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: SEND TO ELASTICSEARCH (HTTP Request)                â”‚
â”‚ POST http://localhost:9200/multimedia/_search               â”‚
â”‚ {                                                            â”‚
â”‚   "knn": {                                                   â”‚
â”‚     "field": "embedding",                                    â”‚
â”‚     "query_vector": [0.234, -0.123, ..., 0.456],           â”‚
â”‚     "k": 10                                                  â”‚
â”‚   }                                                          â”‚
â”‚ }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: ES CLUSTER SEARCH (Distributed)                     â”‚
â”‚                                                              â”‚
â”‚  [es01 - Shard 0]     [es02 - Shard 1]     [es03 - Shard 2]â”‚
â”‚        â†“                     â†“                     â†“         â”‚
â”‚   Search 337 docs       Search 337 docs       Search 336    â”‚
â”‚   HNSW algorithm        HNSW algorithm        HNSW algo     â”‚
â”‚   Find top 10           Find top 10           Find top 10   â”‚
â”‚        â†“                     â†“                     â†“         â”‚
â”‚   [video_3: 0.94]      [video_7: 0.91]      [image_234]    â”‚
â”‚   [image_12: 0.89]     [image_45: 0.87]     [audio_56]     â”‚
â”‚   ...                  ...                   ...            â”‚
â”‚        â†“                     â†“                     â†“         â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                            â†“                                 â”‚
â”‚                  [es01 - Coordinator]                        â”‚
â”‚                  Merge 30 results                            â”‚
â”‚                  Sort by score                               â”‚
â”‚                  Return top 10                               â”‚
â”‚                  Time: 74ms                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: RETURN TO CLIENT (HTTP Response)                    â”‚
â”‚ {                                                            â”‚
â”‚   "hits": {                                                  â”‚
â”‚     "total": {"value": 1010},                               â”‚
â”‚     "hits": [                                                â”‚
â”‚       {                                                      â”‚
â”‚         "_score": 0.94,                                      â”‚
â”‚         "_source": {                                         â”‚
â”‚           "filename": "pexels_3571264.mp4",                 â”‚
â”‚           "media_type": "video",                             â”‚
â”‚           "file_path": "data/raw/pexels_videos/..."         â”‚
â”‚         }                                                    â”‚
â”‚       },                                                     â”‚
â”‚       ...                                                    â”‚
â”‚     ]                                                        â”‚
â”‚   }                                                          â”‚
â”‚ }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: DISPLAY TO USER (Console Output)                    â”‚
â”‚                                                              â”‚
â”‚ ğŸ” Search Results for "ocean waves":                        â”‚
â”‚                                                              â”‚
â”‚ 1. pexels_3571264.mp4 (video) - â­â­â­â­â­                   â”‚
â”‚    Similarity: 0.940                                         â”‚
â”‚    Path: data/raw/pexels_videos/pexels_3571264.mp4          â”‚
â”‚                                                              â”‚
â”‚ 2. image_0234.jpg (image) - â­â­â­â­                         â”‚
â”‚    Similarity: 0.891                                         â”‚
â”‚                                                              â”‚
â”‚ 3. pexels_2169307.mp4 (video) - â­â­â­â­                     â”‚
â”‚    Similarity: 0.872                                         â”‚
â”‚                                                              â”‚
â”‚ Total time: 89ms (encoding 15ms + search 74ms)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ”§ **MONITORING & FAILOVER**

**Scenario: Node es02 bá»‹ crash**

```
BEFORE CRASH:
Node es01: [Primary-0, Replica-2] â† Coordinator
Node es02: [Primary-1, Replica-0] â† CRASHED! ğŸ’¥
Node es03: [Primary-2, Replica-1]

AFTER CRASH (Auto Failover):
1. ES detects node failure (~3 seconds)
2. Promote Replica-0 (on es01) â†’ Primary-0
3. Promote Replica-1 (on es03) â†’ Primary-1
4. Cluster status: YELLOW (missing replicas)
5. Searches still work! (all primaries available)

Node es01: [Primary-0, Primary-0*, Replica-2]
Node es03: [Primary-2, Primary-1*, Replica-1]

WHEN es02 RESTARTS:
6. Rejoin cluster
7. Create new replicas
8. Cluster status: GREEN âœ…

Node es01: [Primary-0, Replica-2]
Node es02: [Replica-0, Replica-1] â† Back online
Node es03: [Primary-2, Primary-1]
```

---

### ğŸ’¡ **ÄIá»‚M QUAN TRá»ŒNG KHI GIáº¢I THÃCH**

**1. Táº¡i sao cáº§n 2 giai Ä‘oáº¡n (Offline/Online)?**
> "Offline phase tá»‘n thá»i gian (CLIP encoding), nhÆ°ng chá»‰ cháº¡y 1 láº§n khi setup. Online phase pháº£i nhanh (<100ms) Ä‘á»ƒ user experience tá»‘t. Separation of concerns cho phÃ©p optimize riÃªng tá»«ng phase."

**2. CLIP model vai trÃ² gÃ¬?**
> "CLIP lÃ  'translator' - chuyá»ƒn text/image tá»« dáº¡ng raw thÃ nh vector 512-d. Sau khi encode xong, search lÃ  pure math (cosine similarity). CLIP chá»‰ cháº¡y lÃºc encode query vÃ  encode dataset láº§n Ä‘áº§u."

**3. Elasticsearch lÃ m gÃ¬?**
> "ES lÃ  'search engine' - store vectors vÃ  tÃ¬m kiáº¿m nhanh báº±ng HNSW algorithm. ES khÃ´ng hiá»ƒu semantic, chá»‰ so sÃ¡nh numbers. Semantic understanding do CLIP Ä‘áº£m nhiá»‡m."

**4. Táº¡i sao distributed?**
> "1,010 docs cÃ³ thá»ƒ cháº¡y 1 node, nhÆ°ng khi scale lÃªn millions, 1 node khÃ´ng Ä‘á»§ RAM/CPU. Distribute ngay tá»« Ä‘áº§u Ä‘á»ƒ prove architecture scale Ä‘Æ°á»£c. 3 shards search parallel â†’ latency khÃ´ng tÄƒng khi data tÄƒng (váº«n ~74ms)."

**5. Náº¿u thÃªm 1,000 documents má»›i?**
```python
# Index thÃªm realtime
new_docs = [...]  # 1,000 new documents with embeddings
bulk(es, new_docs)

# ES tá»± Ä‘á»™ng:
# 1. Hash document_id â†’ determine shard (0, 1, or 2)
# 2. Add to appropriate shard
# 3. Replicate to replica shard
# 4. Update HNSW graph
# 5. Available for search sau ~1 giÃ¢y

# Search performance: Váº«n ~74ms (HNSW scales O(log N))
```

---

## 3. CÃ‚U Há»I Vá»€ Äáº¶T Váº¤N Äá»€ & Má»¤C TIÃŠU

### â“ **"Táº¡i sao chá»n Ä‘á» tÃ i nÃ y?"**

**Tráº£ lá»i:**

> "Em chá»n Ä‘á» tÃ i nÃ y vÃ¬ 3 lÃ½ do chÃ­nh:
>
> 1. **Thá»±c táº¿:** Dá»¯ liá»‡u Ä‘a phÆ°Æ¡ng tiá»‡n Ä‘ang bÃ¹ng ná»• (hÃ ng tá»· images/videos má»—i ngÃ y trÃªn YouTube, Instagram), nhÆ°ng tÃ¬m kiáº¿m truyá»n thá»‘ng chá»‰ dá»±a vÃ o metadata/tags - khÃ´ng hiá»ƒu ná»™i dung thá»±c sá»±.
>
> 2. **Ká»¹ thuáº­t:** Káº¿t há»£p Ä‘Æ°á»£c nhiá»u cÃ´ng nghá»‡ Big Data quan trá»ng: Elasticsearch (distributed search), Docker (containerization), AI (CLIP model), Python (data processing).
>
> 3. **á»¨ng dá»¥ng:** CÃ³ thá»ƒ Ã¡p dá»¥ng vÃ o nhiá»u lÄ©nh vá»±c thá»±c táº¿ nhÆ° e-commerce (tÃ¬m sáº£n pháº©m tá»« áº£nh), media platforms (tÃ¬m video tÆ°Æ¡ng tá»±), security (face recognition)."

### â“ **"KhÃ¡c gÃ¬ vá»›i tÃ¬m kiáº¿m Google Images?"**

**Tráº£ lá»i:**

> "CÃ³ 3 Ä‘iá»ƒm khÃ¡c biá»‡t chÃ­nh:
>
> 1. **Cross-modal:** Há»‡ thá»‘ng em cho phÃ©p tÃ¬m VIDEO tá»« TEXT, tÃ¬m AUDIO tá»« IMAGE - Google Images chá»§ yáº¿u text â†’ image.
>
> 2. **Self-hosted:** Em triá»ƒn khai trÃªn infrastructure riÃªng, kiá»ƒm soÃ¡t hoÃ n toÃ n data vÃ  model - phÃ¹ há»£p cho doanh nghiá»‡p cáº§n báº£o máº­t.
>
> 3. **Customizable:** CÃ³ thá»ƒ fine-tune model cho domain cá»¥ thá»ƒ, Ä‘iá»u chá»‰nh ranking algorithm - Google lÃ  black box.
>
> 4. **Distributed by design:** Elasticsearch cluster 3 nodes sáºµn sÃ ng scale horizontal khi data tÄƒng lÃªn GB-TB."

### â“ **"Má»¥c tiÃªu Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng gÃ¬?"**

**Tráº£ lá»i:**

> "Em Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c Ä‘áº§y Ä‘á»§ 4 má»¥c tiÃªu Ä‘á» ra:
>
> âœ… **Dataset:** 1,010 files multimedia (900MB) - bao gá»“m 10 real HD videos cháº¥t lÆ°á»£ng cao tá»« Pexels
>
> âœ… **AI Embeddings:** TÃ­ch há»£p CLIP model, táº¡o 512-d vectors cho toÃ n bá»™ dataset
>
> âœ… **Distributed System:** Elasticsearch cluster 3 nodes, GREEN status, 100% active shards
>
> âœ… **Performance:** Latency 74ms (< 100ms má»¥c tiÃªu), throughput 13.4 QPS (> 10 QPS má»¥c tiÃªu)"

---

## 3. CÃ‚U Há»I Vá»€ CÃ”NG NGHá»†

### â“ **"Táº¡i sao chá»n Elasticsearch thay vÃ¬ MySQL/MongoDB?"**

**Tráº£ lá»i:**

> "Em chá»n Elasticsearch vÃ¬ 4 lÃ½ do ká»¹ thuáº­t:
>
> 1. **Vector Search Native:** Tá»« version 7.3, ES há»— trá»£ dense_vector field type vá»›i HNSW algorithm - tÃ¬m kiáº¿m vector nhanh O(log N) thay vÃ¬ O(N) brute-force.
>
> 2. **Distributed by Default:** ES tá»± Ä‘á»™ng shard vÃ  replicate data. MySQL cáº§n manual sharding, MongoDB replica set phá»©c táº¡p hÆ¡n.
>
> 3. **Near Real-time:** Data cÃ³ thá»ƒ search sau ~1 giÃ¢y index. MongoDB fast nhÆ°ng khÃ´ng specialized cho search.
>
> 4. **Proven at Scale:** LinkedIn, Uber, Netflix dÃ¹ng ES cho billions documents. ÄÃ£ battle-tested cho Big Data."

**So sÃ¡nh cá»¥ thá»ƒ:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature         â”‚ Elasticsearchâ”‚ MongoDB      â”‚ MySQL        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Vector Search   â”‚ Native       â”‚ Plugin       â”‚ Manual UDF   â”‚
â”‚ Sharding        â”‚ Auto         â”‚ Manual       â”‚ Manual       â”‚
â”‚ Full-text       â”‚ Excellent    â”‚ Basic        â”‚ Limited      â”‚
â”‚ Horizontal Scaleâ”‚ Easy         â”‚ Medium       â”‚ Hard         â”‚
â”‚ Use case        â”‚ Search       â”‚ Document DB  â”‚ Relational   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### â“ **"CLIP model lÃ  gÃ¬? Táº¡i sao dÃ¹ng nÃ³?"**

**Tráº£ lá»i:**

> "CLIP (Contrastive Language-Image Pre-training) lÃ  mÃ´ hÃ¬nh AI cá»§a OpenAI (2021) vá»›i 3 Ä‘áº·c Ä‘iá»ƒm quan trá»ng:
>
> 1. **Multimodal:** Hiá»ƒu cáº£ TEXT vÃ  IMAGE trong cÃ¹ng 1 embedding space â†’ cÃ³ thá»ƒ so sÃ¡nh text vá»›i image trá»±c tiáº¿p.
>
> 2. **Pre-trained:** ÄÆ°á»£c train trÃªn 400 million text-image pairs tá»« internet â†’ hiá»ƒu ngá»¯ nghÄ©a tá»‘t mÃ  khÃ´ng cáº§n fine-tune.
>
> 3. **Zero-shot:** CÃ³ thá»ƒ classify/search cÃ¡c objects chÆ°a tháº¥y bao giá» - chá»‰ cáº§n mÃ´ táº£ báº±ng text.
>
> **VÃ­ dá»¥ thá»±c táº¿:** Query 'a cat sleeping' â†’ CLIP encode thÃ nh vector â†’ tÃ¬m images/videos cÃ³ vector tÆ°Æ¡ng tá»± â†’ tráº£ vá» áº£nh mÃ¨o ngá»§, dÃ¹ khÃ´ng cÃ³ tag 'cat' hay 'sleeping'."

**Cáº¥u trÃºc CLIP:**

- Text Encoder: Transformer (12 layers) â†’ 512-d vector
- Image Encoder: Vision Transformer (ViT-B/32) â†’ 512-d vector
- Training: Contrastive learning (maximize similarity cá»§a cáº·p Ä‘Ãºng, minimize cáº·p sai)

### â“ **"Táº¡i sao dÃ¹ng Docker? CÃ i trá»±c tiáº¿p khÃ´ng Ä‘Æ°á»£c sao?"**

**Tráº£ lá»i:**

> "Docker giáº£i quyáº¿t 4 váº¥n Ä‘á» thá»±c táº¿:
>
> 1. **Consistency:** 'It works on my machine' problem - Docker Ä‘áº£m báº£o mÃ´i trÆ°á»ng giá»‘ng há»‡t trÃªn dev/prod.
>
> 2. **Isolation:** Elasticsearch, Kibana, Solr cháº¡y Ä‘á»™c láº­p - khÃ´ng conflict dependencies.
>
> 3. **Scalability:** ThÃªm node má»›i chá»‰ cáº§n `docker-compose scale es=5` - khÃ´ng cáº§n cÃ i Ä‘áº·t láº¡i.
>
> 4. **Portability:** ÄÃ³ng gÃ³i toÃ n bá»™ há»‡ thá»‘ng thÃ nh images â†’ deploy anywhere (cloud, on-premise)."

---

## 4. CÃ‚U Há»I Vá»€ DATASET

### â“ **"Táº¡i sao dataset chá»‰ 900MB, khÃ´ng lá»›n hÆ¡n?"**

**Tráº£ lá»i:**

> "Em cÃ³ lÃ½ do ká»¹ thuáº­t vÃ  thá»±c tiá»…n:
>
> **Ká»¹ thuáº­t:**
>
> - 900MB Ä‘á»§ Ä‘á»ƒ demonstrate distributed system (3 nodes, 6 shards)
> - Vector embeddings (512-d Ã— 1,010 = ~2MB) nhá» hÆ¡n nhiá»u so vá»›i raw data â†’ scaling strategy lÃ  quan trá»ng hÆ¡n size tuyá»‡t Ä‘á»‘i
> - Vá»›i kiáº¿n trÃºc hiá»‡n táº¡i, scale lÃªn 10GB-100GB chá»‰ cáº§n thÃªm nodes vÃ  storage
>
> **Thá»±c tiá»…n:**
>
> - Dataset ban Ä‘áº§u 1.75GB â†’ tá»‘i Æ°u xuá»‘ng 900MB Ä‘á»ƒ dá»… share qua Google Drive
> - Focus vÃ o quality: 10 real HD videos tá»« Pexels thay vÃ¬ hÃ ng trÄƒm videos synthetic kÃ©m cháº¥t lÆ°á»£ng
> - Classroom environment: nhiá»u nhÃ³m cháº¡y cÃ¹ng lÃºc trÃªn mÃ¡y lab
>
> **Minh chá»©ng scalability:**
>
> - Elasticsearch Ä‘Ã£ proven scale Ä‘áº¿n petabytes (Uber, Netflix)
> - Em Ä‘Ã£ config sharding ratio 3:1 (3 primary) - best practice cho production"

### â“ **"Táº¡i sao pháº§n lá»›n lÃ  synthetic data?"**

**Tráº£ lá»i:**

> "Em sá»­ dá»¥ng synthetic data cÃ³ kiá»ƒm soÃ¡t vÃ¬:
>
> 1. **Copyright:** TrÃ¡nh váº¥n Ä‘á» báº£n quyá»n khi demo/ná»™p bÃ i. Real data (videos) em dÃ¹ng tá»« Pexels Free License.
>
> 2. **Controlled Testing:** Synthetic images cÃ³ labels rÃµ rÃ ng â†’ dá»… verify káº¿t quáº£ search cÃ³ Ä‘Ãºng khÃ´ng.
>
> 3. **Scalability Demo:** Chá»©ng minh cÃ³ thá»ƒ generate data tá»± Ä‘á»™ng â†’ scale lÃªn millions items khi cáº§n.
>
> **NhÆ°ng váº«n cÃ³ real data:**
>
> - 10 HD videos (738MB/900MB = 82% dataset) lÃ  REAL tá»« Pexels
> - Videos má»›i lÃ  thá»­ thÃ¡ch lá»›n: pháº£i extract keyframes, process temporal data
> - Em chá»n trade-off: quality over quantity"

### â“ **"LÃ m sao xá»­ lÃ½ video thÃ nh embeddings?"**

**Tráº£ lá»i:**

> "Em sá»­ dá»¥ng ká»¹ thuáº­t keyframe extraction:
>
> **BÆ°á»›c 1: Extract Keyframes (OpenCV)**
>
> ```python
> # Láº¥y 1 frame má»—i 30 frames
> cap = cv2.VideoCapture(video_path)
> frame_count = 0
> keyframes = []
>
> while cap.isOpened():
>     ret, frame = cap.read()
>     if not ret: break
>     if frame_count % 30 == 0:  # Every 1 second at 30fps
>         keyframes.append(frame)
>     frame_count += 1
> ```
>
> **BÆ°á»›c 2: Encode Each Keyframe**
>
> - Má»—i keyframe â†’ CLIP Image Encoder â†’ 512-d vector
>
> **BÆ°á»›c 3: Aggregate**
>
> - Average pooling: vector_video = mean(all keyframe vectors)
> - Hoáº·c max pooling: láº¥y max value má»—i dimension
>
> **Káº¿t quáº£:** 1 video (vÃ i giÃ¢y) â†’ 1 vector 512-d Ä‘áº¡i diá»‡n cho ná»™i dung chÃ­nh."

### â“ **"Audio thÃ¬ sao? CLIP khÃ´ng há»— trá»£ audio mÃ ?"**

**Tráº£ lá»i:**

> "ÄÃºng, CLIP chá»‰ support text vÃ  image. Em giáº£i quyáº¿t báº±ng cÃ¡ch chuyá»ƒn audio â†’ image:
>
> **Spectrogram Approach:**
>
> ```python
> # Audio waveform â†’ Spectrogram (time-frequency representation)
> import librosa
> import matplotlib.pyplot as plt
>
> # Load audio
> y, sr = librosa.load(audio_path)
>
> # Generate mel spectrogram
> S = librosa.feature.melspectrogram(y=y, sr=sr)
> S_db = librosa.power_to_db(S, ref=np.max)
>
> # Save as image
> plt.figure(figsize=(10, 4))
> librosa.display.specshow(S_db)
> plt.savefig('spectrogram.png')
>
> # Now use CLIP image encoder
> spectrogram_image = Image.open('spectrogram.png')
> embedding = clip_model.encode_image(spectrogram_image)
> ```
>
> **Ã tÆ°á»Ÿng:** Spectrogram lÃ  visual representation cá»§a audio â†’ CLIP cÃ³ thá»ƒ process nhÆ° image bÃ¬nh thÆ°á»ng.
>
> **Háº¡n cháº¿:** KhÃ´ng tá»‘i Æ°u báº±ng model chuyÃªn cho audio (nhÆ° Wav2Vec), nhÆ°ng demonstrate Ä‘Æ°á»£c cross-modal capability."

---

## 5. CÃ‚U Há»I Vá»€ KIáº¾N TRÃšC Há»† THá»NG

### â“ **"Giáº£i thÃ­ch kiáº¿n trÃºc 3 táº§ng?"**

**Tráº£ lá»i:**

> "Há»‡ thá»‘ng em theo kiáº¿n trÃºc 3-tier chuáº©n enterprise:
>
> **Tier 1 - User Layer:**
>
> - `demo_multimodal_search.py`: Python CLI interactive
> - Chá»©c nÄƒng: Text search, Image search, Cross-modal, Performance test, Health check
> - Giao tiáº¿p vá»›i Elasticsearch qua REST API (HTTP)
>
> **Tier 2 - Processing & Storage Layer:**
>
> - **Elasticsearch Cluster:** 3 nodes (es01:9200, es02:9201, es03:9202)
>   - es01: Master + Data node
>   - es02, es03: Data nodes
>   - Sharding: 3 primary + 3 replica
> - **Kibana:** Monitoring dashboard (port 5601)
> - **Solr:** Comparison purpose (port 8983)
>
> **Tier 3 - Data Layer:**
>
> - Raw data: 900MB multimedia files (images/, pexels_videos/, audios/)
> - Embeddings: .npy files chá»©a 512-d vectors
> - CLIP Model: Pre-trained weights
>
> **Luá»“ng hoáº¡t Ä‘á»™ng:**
> User query â†’ Encode báº±ng CLIP â†’ Vector search trong ES â†’ Return top-k results"

### â“ **"Táº¡i sao cáº§n 3 nodes? 1 node khÃ´ng Ä‘á»§ sao?"**

**Tráº£ lá»i:**

> "3 nodes demonstrate distributed system properly:
>
> **1. High Availability:**
>
> - Náº¿u 1 node die â†’ replica shards trÃªn 2 nodes cÃ²n láº¡i Ä‘Æ°á»£c promote lÃªn primary
> - Cluster váº«n GREEN, khÃ´ng downtime
> - 1 node = single point of failure
>
> **2. Load Balancing:**
>
> - Queries distributed across 3 nodes â†’ throughput tÄƒng ~3x
> - Má»—i node xá»­ lÃ½ 1/3 data (3 primary shards)
>
> **3. Realistic Architecture:**
>
> - Production ES clusters thÆ°á»ng â‰¥3 nodes (Netflix: hÃ ng trÄƒm nodes)
> - 3 lÃ  minimum Ä‘á»ƒ demonstrate master election (quorum = N/2 + 1 = 2)
>
> **4. Scalability Testing:**
>
> - ThÃªm node thá»© 4,5 ráº¥t dá»…: `docker-compose scale es=5`
> - Data tá»± Ä‘á»™ng rebalance
>
> **Trade-off:** 3 nodes tá»‘n RAM hÆ¡n (má»—i node ~1.5GB heap = 4.5GB total), nhÆ°ng demonstrate Ä‘Æ°á»£c distributed concepts."

### â“ **"Sharding vÃ  Replication hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?"**

**Tráº£ lá»i:**

> "Em config 3 primary shards + 3 replica shards:
>
> **Sharding (Horizontal Partitioning):**
>
> - 1,010 documents chia Ä‘á»u cho 3 primary shards
> - Má»—i shard: ~337 documents
> - Routing: document_id â†’ hash â†’ shard number (0, 1, hoáº·c 2)
> - Parallel search: 3 shards search Ä‘á»“ng thá»i â†’ merge results
>
> **Replication (Backup):**
>
> - Má»—i primary shard cÃ³ 1 replica
> - Replica KHÃ”NG náº±m cÃ¹ng node vá»›i primary (anti-affinity)
>   -VÃ­ dá»¥ phÃ¢n bá»‘:
>   ```
>   Node es01: Primary 0, Replica 2
>   Node es02: Primary 1, Replica 0
>   Node es03: Primary 2, Replica 1
>   ```
>
> **Failover Scenario:**
>
> 1. es01 die â†’ Primary 0 máº¥t
> 2. ES auto promote Replica 0 (trÃªn es02) lÃªn Primary 0
> 3. Cluster váº«n hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng
> 4. Khi es01 restart â†’ táº¡o Replica 0 má»›i
>
> **Best Practice:**
>
> - Number of primaries = expected data growth / shard size target (30-50GB/shard)
> - Number of replicas â‰¥ 1 cho production"

---

## 6. CÃ‚U Há»I Vá»€ TRIá»‚N KHAI

### â“ **"Quy trÃ¬nh triá»ƒn khai tá»«ng bÆ°á»›c?"**

**Tráº£ lá»i:**

> "Em triá»ƒn khai theo 5 bÆ°á»›c:
>
> **BÆ°á»›c 1: Thu tháº­p dá»¯ liá»‡u (~30 phÃºt)**
>
> - Download 10 real videos tá»« Pexels API
> - Generate 800 images synthetic (PIL - Python Imaging Library)
> - Generate 200 audios synthetic (scipy waveform)
> - Total: 911.88 MB
>
> **BÆ°á»›c 2: Táº¡o embeddings (~10 phÃºt vá»›i CPU)**
>
> - Load CLIP model: `clip.load('ViT-B/32')`
> - Process images: 800 Ã— 512-d vectors
> - Process videos: Extract keyframes â†’ encode â†’ average â†’ 10 Ã— 512-d
> - Process audios: Spectrogram â†’ encode â†’ 200 Ã— 512-d
> - Save: .npy format (numpy arrays)
>
> **BÆ°á»›c 3: CÃ i Ä‘áº·t cluster (~5 phÃºt)**
>
> - `docker-compose -f docker-compose-cluster.yml up -d`
> - 3 ES nodes + Kibana + Solr start
> - Wait ~30-60s cho cluster GREEN
>
> **BÆ°á»›c 4: Indexing (~30 giÃ¢y)**
>
> - Create index vá»›i dense_vector mapping (512 dims, cosine similarity)
> - Bulk index 1,010 documents (embeddings + metadata)
> - Verify: 6/6 shards active
>
> **BÆ°á»›c 5: Demo & Testing (~5 phÃºt)**
>
> - Run demo: `python demo_multimodal_search.py`
> - Test 5 features
> - Benchmark: 74ms latency, 13.4 QPS
>
> **Total: ~2 hours** (bao gá»“m download, debug, testing)"

### â“ **"Gáº·p khÃ³ khÄƒn gÃ¬ khi triá»ƒn khai?"**

**Tráº£ lá»i (thÃ nh tháº­t nhÆ°ng cÃ³ giáº£i phÃ¡p):**

> "Em gáº·p 3 váº¥n Ä‘á» chÃ­nh:
>
> **1. Docker Memory Issues:**
>
> - **Váº¥n Ä‘á»:** 3 ES nodes tá»‘n ~4.5GB RAM â†’ laptop 8GB bá»‹ treo
> - **Giáº£i phÃ¡p:** Giáº£m heap size xuá»‘ng `-Xms256m -Xmx512m` má»—i node â†’ cháº¡y Ä‘Æ°á»£c nhÆ°ng cháº­m hÆ¡n
> - **Há»c Ä‘Æ°á»£c:** Production cáº§n plan capacity ká»¹ (ES recommend 8GB heap/node)
>
> **2. CLIP Model Download:**
>
> - **Váº¥n Ä‘á»:** Model 350MB, máº¡ng lab cháº­m â†’ timeout
> - **Giáº£i phÃ¡p:** Download manual vá» ~/.cache/huggingface/, share cho cáº£ nhÃ³m
> - **Há»c Ä‘Æ°á»£c:** LuÃ´n cache models, khÃ´ng download má»—i láº§n cháº¡y
>
> **3. Video Processing Slow:**
>
> - **Váº¥n Ä‘á»:** 10 videos xá»­ lÃ½ máº¥t 1-2 phÃºt (CPU only)
> - **Giáº£i phÃ¡p:**
>   - Giáº£m keyframe sampling: 1 frame/second thay vÃ¬ má»—i frame
>   - Parallel processing: multiprocessing.Pool
> - **Há»c Ä‘Æ°á»£c:** GPU sáº½ nhanh hÆ¡n ~10x, nhÆ°ng khÃ´ng available trong lab
>
> **Táº¥t cáº£ Ä‘á»u solved vÃ  documented trong README.md**"

### â“ **"LÃ m sao Ä‘áº£m báº£o cluster status GREEN?"**

**Tráº£ lá»i:**

> "Em check vÃ  maintain GREEN status qua 4 cÃ¡ch:
>
> **1. Health Check API:**
>
> ```bash
> curl http://localhost:9200/_cluster/health?pretty
> ```
>
> Output cáº§n tháº¥y: `\"status\": \"green\"`
>
> **2. Shard Allocation:**
>
> - Verify all 6 shards STARTED:
>
> ```bash
> curl http://localhost:9200/_cat/shards/multimedia?v
> ```
>
> - 3 primary + 3 replica = 6 total
> - KhÃ´ng cÃ³ UNASSIGNED shards
>
> **3. Node Health:**
>
> ```bash
> curl http://localhost:9200/_cat/nodes?v
> ```
>
> - 3/3 nodes online
> - Heap usage < 75%
> - CPU < 80%
>
> **4. Kibana Monitoring:**
>
> - http://localhost:5601
> - Dashboard real-time: cluster status, node stats, index stats
>
> **Troubleshooting náº¿u YELLOW/RED:**
>
> - YELLOW: Replica shards chÆ°a assigned â†’ thÆ°á»ng do thiáº¿u nodes
> - RED: Primary shards missing â†’ nghiÃªm trá»ng, data loss
> - Fix: Restart nodes, rebalance shards, increase replicas"

---

## 7. CÃ‚U Há»I Vá»€ Káº¾T QUáº¢

### â“ **"Performance 74ms latency cÃ³ tá»‘t khÃ´ng?"**

**Tráº£ lá»i:**

> "74ms lÃ  **ráº¥t tá»‘t** cho vector search:
>
> **So sÃ¡nh industry:**
>
> - Google search: ~200-500ms (nhÆ°ng search global scale)
> - E-commerce search (Amazon): ~100-200ms
> - Em Ä‘áº¡t 74ms vá»›i 1,010 documents â†’ scale lÃªn millions váº«n maintain ~100-200ms vá»›i HNSW
>
> **Breakdown latency:**
>
> - Network overhead: ~5ms
> - Query parsing: ~2ms
> - Vector similarity (HNSW): ~60ms
> - Result aggregation: ~7ms
>
> **Faster than Solr:**
>
> - Solr: 82ms (10% cháº­m hÆ¡n)
> - Elasticsearch HNSW algorithm tá»‘i Æ°u hÆ¡n Solr vector plugin
>
> **Má»¥c tiÃªu Ä‘áº·t ra:** < 100ms â†’ **Äáº¡t âœ“**
>
> **Improvement cÃ³ thá»ƒ:**
>
> - Cache frequently queried vectors â†’ ~30-40ms
> - GPU acceleration â†’ ~10-20ms
> - SSD thay vÃ¬ HDD â†’ ~50ms"

### â“ **"13.4 QPS cÃ³ Ä‘á»§ khÃ´ng? Production cáº§n bao nhiÃªu?"**

**Tráº£ lá»i:**

> "13.4 QPS lÃ  **baseline tá»‘t** cho prototype:
>
> **Context:**
>
> - 13.4 queries/second = 804 queries/minute = 48,240 queries/hour
> - Vá»›i 1,000 users concurrent, má»—i user query 1 láº§n/minute â†’ cáº§n ~16.7 QPS
> - Em Ä‘áº¡t 13.4 QPS vá»›i **1 client thread** â†’ chÆ°a optimize
>
> **Production requirements:**
>
> - Small site (< 10K DAU): 10-50 QPS
> - Medium site (100K DAU): 100-500 QPS
> - Large site (1M+ DAU): 1000+ QPS
>
> **Scaling strategy:**
>
> - **Horizontal:** ThÃªm nodes 3 â†’ 6 â†’ QPS double (~26 QPS)
> - **Vertical:** TÄƒng CPU/RAM má»—i node â†’ +30-50% QPS
> - **Caching:** Redis cache hot queries â†’ reduce load 50-70%
> - **Load balancer:** Nginx phÃ¢n tÃ¡n requests â†’ tÄƒng throughput
>
> **Thá»±c táº¿:**
>
> - Netflix ES cluster: hÃ ng nghÃ¬n QPS
> - Uber: hÃ ng chá»¥c nghÃ¬n QPS
> - Em demonstrate Ä‘Æ°á»£c scalability path rÃµ rÃ ng"

### â“ **"Elasticsearch tá»‘t hÆ¡n Solr á»Ÿ Ä‘iá»ƒm nÃ o?"**

**Tráº£ lá»i (cÃ³ sá»‘ liá»‡u cá»¥ thá»ƒ):**

> "Em Ä‘Ã£ benchmark cáº£ 2 há»‡ thá»‘ng, káº¿t quáº£:
>
> **Performance (ES wins):**
>
> ```
> Latency avg:    ES 74ms  vs Solr 82ms   (+10% faster)
> P95 latency:    ES 102ms vs Solr 115ms  (+13% faster)
> QPS:            ES 13.4  vs Solr 12.2   (+10% higher)
> ```
>
> **Architecture (ES wins):**
>
> - ES: Distributed by default, auto sharding
> - Solr: SolrCloud cáº§n manual config, phá»©c táº¡p hÆ¡n
>
> **Developer Experience (ES wins):**
>
> - ES: JSON API, RESTful, dá»… dÃ¹ng
> - Solr: XML config files, learning curve dÃ i
>
> **Vector Search (ES wins):**
>
> - ES: Native dense_vector tá»« v7.3, HNSW algorithm built-in
> - Solr: Cáº§n plugin (solr-vector), khÃ´ng stable báº±ng
>
> **DevOps (ES wins):**
>
> - ES: Docker official images, Kubernetes operators
> - Solr: Setup phá»©c táº¡p hÆ¡n, Ã­t tooling
>
> **Khi nÃ o dÃ¹ng Solr:**
>
> - ÄÃ£ cÃ³ Hadoop ecosystem (Solr integrate tá»‘t)
> - Cáº§n advanced faceting (Solr máº¡nh hÆ¡n)
> - Legacy system Ä‘Ã£ dÃ¹ng Solr
>
> **Káº¿t luáº­n:** ES phÃ¹ há»£p hÆ¡n cho multimodal vector search Big Data"

### â“ **"Demo cÃ³ gÃ¬ Ä‘áº·c biá»‡t?"**

**Tráº£ lá»i:**

> "Demo cá»§a em cÃ³ 5 features unique:
>
> **1. Text-to-Media Search:**
>
> - Input: 'a person playing guitar'
> - Output: Relevant images, videos, audios ranked by similarity
> - Highlight: Semantic search, khÃ´ng cáº§n exact keyword match
>
> **2. Image-to-Media Search:**
>
> - Input: Upload áº£nh báº¥t ká»³
> - Output: Similar media across all types
> - Highlight: Visual similarity, content-based retrieval
>
> **3. Cross-Modal Search:**
>
> - Text â†’ find Videos
> - Image â†’ find Audios (via spectrogram similarity)
> - Highlight: CLIP unified embedding space
>
> **4. Performance Testing:**
>
> - Run 100 random queries
> - Measure: avg latency, P95, P99, QPS
> - Real-time dashboard display
> - Highlight: Transparency, reproducible benchmarks
>
> **5. Health Check:**
>
> - Cluster status (GREEN/YELLOW/RED)
> - Node health (CPU, heap, disk)
> - Shard distribution visualization
> - Highlight: Production-ready monitoring
>
> **Äáº·c biá»‡t:** Táº¥t cáº£ cháº¡y real-time, khÃ´ng fake data, cÃ³ thá»ƒ reproduce 100%"

---

## 8. CÃ‚U Há»I KHÃ“ & CÃCH TRáº¢ Lá»œI

### â“ **"Há»‡ thá»‘ng cá»§a em cÃ³ thá»ƒ thay tháº¿ Google khÃ´ng?"**

**Tráº£ lá»i (khÃ©o lÃ©o):**

> "KhÃ´ng thá»ƒ thay tháº¿ Google vÃ¬ scope khÃ¡c nhau, nhÆ°ng cÃ³ thá»ƒ Ã¡p dá»¥ng trong enterprise:
>
> **KhÃ¡c biá»‡t:**
>
> - Google: Search toÃ n bá»™ internet (billions pages), multi-language, multi-modal
> - Em: Search private data (company assets), specific domain
>
> **Æ¯u tháº¿ cá»§a em trong enterprise:**
>
> 1. **Privacy:** Data khÃ´ng rá»i khá»i infrastructure cÃ´ng ty
> 2. **Customization:** Fine-tune model cho domain cá»¥ thá»ƒ (medical images, fashion products)
> 3. **Cost:** Self-hosted, khÃ´ng pay per query
> 4. **Control:** Full control ranking algorithm, khÃ´ng bá»‹ ads áº£nh hÆ°á»Ÿng
>
> **VÃ­ dá»¥ use case:**
>
> - **E-commerce:** Shopee search sáº£n pháº©m tá»« áº£nh upload
> - **Media company:** VTV search archive videos báº±ng mÃ´ táº£ text
> - **Hospital:** Search medical images by symptoms description
>
> **Káº¿t luáº­n:** KhÃ´ng compete vá»›i Google, nhÆ°ng solve different problems"

### â“ **"Scale lÃªn 1TB data thÃ¬ sao? Há»‡ thá»‘ng cÃ³ chá»‹u Ä‘Æ°á»£c khÃ´ng?"**

**Tráº£ lá»i:**

> "Há»‡ thá»‘ng em thiáº¿t káº¿ sáºµn cho scalability:
>
> **Calculations:**
>
> - Current: 1,010 docs, 911MB raw, 2MB embeddings, 3 shards
> - 1TB = ~1,100,000 docs (scale ~1000x)
> - Embeddings: ~2GB (váº«n ráº¥t nhá» so vá»›i raw data)
>
> **Scaling plan:**
>
> **Storage:**
>
> - Current: 3 nodes Ã— 1GB = 3GB total
> - 1TB: Cáº§n ~10-15 nodes (má»—i node 100GB)
> - ES recommend: 30-50GB/shard â†’ ~30-40 shards
>
> **Sharding strategy:**
>
> - Increase primary shards: 3 â†’ 30 (10x)
> - Keep replica: 1 (HA requirement)
> - Total shards: 60 (30 primary + 30 replica)
>
> **Performance impact:**
>
> - Query latency: 74ms â†’ ~100-150ms (HNSW scales O(log N))
> - Index time: Parallel indexing across 30 shards
> - Storage: SSD required (HDD quÃ¡ cháº­m cho vector search)
>
> **Real-world example:**
>
> - Pinterest: Billions images, ES cluster hÃ ng trÄƒm nodes
> - Uber: HÃ ng TB data, ES handle smooth
>
> **Bottleneck:**
>
> - Network bandwidth (nodes communicate)
> - RAM (má»—i node cáº§n 8-16GB heap)
> - Disk I/O (SSD mandatory)
>
> **Em Ä‘Ã£ prepare:** Docker-compose file há»— trá»£ scale command, sharding config flexible"

### â“ **"Táº¡i sao khÃ´ng dÃ¹ng Faiss (Facebook AI) cho vector search?"**

**Tráº£ lá»i (technical depth):**

> "Faiss tá»‘t hÆ¡n vá» speed, nhÆ°ng ES tá»‘t hÆ¡n vá» complete solution:
>
> **Faiss advantages:**
>
> - Pure vector search, optimize cá»±c ká»³ tá»‘t
> - GPU support tá»‘t â†’ 10-100x faster
> - Nhiá»u index types (IVF, HNSW, PQ)
> - Open source by Facebook/Meta
>
> **Elasticsearch advantages:**
>
> - **Hybrid search:** Vector + full-text + filters trong 1 query
>   ```json
>   {
>     \"query\": {
>       \"bool\": {
>         \"must\": [{\"knn\": {...}}],
>         \"filter\": [{\"range\": {\"date\": {\"gte\": \"2024\"}}}]
>       }
>     }
>   }
>   ```
> - **Distributed:** Faiss cáº§n custom sharding logic
> - **CRUD:** ES support update/delete documents, Faiss rebuild entire index
> - **Monitoring:** Kibana built-in, Faiss cáº§n custom dashboard
> - **Ecosystem:** REST API, drivers cho má»i ngÃ´n ngá»¯
>
> **Khi nÃ o dÃ¹ng Faiss:**
>
> - Pure vector search, khÃ´ng cáº§n filters
> - CÃ³ GPU, cáº§n speed cá»±c cao
> - Batch processing, khÃ´ng cáº§n real-time updates
>
> **Trade-off em chá»n:**
>
> - ES slower 2-3x vs Faiss
> - NhÆ°ng complete platform: search + storage + monitoring + distributed
> - PhÃ¹ há»£p cho production system, khÃ´ng chá»‰ research"

### â“ **"Náº¿u mÃ´ hÃ¬nh CLIP sai, search sáº½ sai háº¿t?"**

**Tráº£ lá»i (acknowledge limitation):**

> "ÄÃºng, Ä‘Ã¢y lÃ  limitation cá»§a embedding-based search:
>
> **Single point of failure:**
>
> - Náº¿u CLIP encode sai â†’ embedding sai â†’ search results sai
> - KhÃ´ng cÃ³ fallback mechanism trong há»‡ thá»‘ng hiá»‡n táº¡i
>
> **VÃ­ dá»¥ CLIP fail:**
>
> - Images cÃ³ text: CLIP confuse vÃ¬ train trÃªn natural images
> - Abstract art: CLIP khÃ´ng hiá»ƒu semantic
> - Domain-specific (medical X-rays): CLIP chÆ°a tháº¥y bao giá»
>
> **Mitigation strategies:**
>
> **1. Hybrid Approach:**
>
> ```json
> {
>   \"query\": {
>     \"bool\": {
>       \"should\": [
>         {\"knn\": {...}},              // Vector search (70% weight)
>         {\"match\": {\"tags\": \"...\"}}, // Traditional search (30%)
>       ]
>     }
>   }
> }
> ```
>
> **2. Ensemble Models:**
>
> - Combine CLIP + ResNet + EfficientNet embeddings
> - Average scores â†’ more robust
>
> **3. Fine-tuning:**
>
> - Fine-tune CLIP on domain data
> - Example: Medical images dataset
>
> **4. Human-in-the-loop:**
>
> - Show top 20 results
> - User feedback â†’ re-rank
> - Active learning
>
> **Em implement:** Hybrid search trong advanced demo (bonus feature)"

---

## 9. DEMO TIPS

### ğŸ¬ **Chuáº©n bá»‹ Demo:**

**TrÆ°á»›c khi demo (15 phÃºt trÆ°á»›c):**

```bash
# 1. Start cluster
cd BigData
docker-compose -f docker-compose-cluster.yml up -d

# 2. Wait for GREEN
sleep 30
curl http://localhost:9200/_cluster/health

# 3. Verify data indexed
curl http://localhost:9200/multimedia/_count
# Should return: {"count": 1010}

# 4. Test search
curl -X POST "http://localhost:9200/multimedia/_search" \
  -H 'Content-Type: application/json' \
  -d '{"size": 1}'

# 5. Open Kibana background tab
# http://localhost:5601
```

**Demo flow (5-7 phÃºt):**

1. **Text Search (1 phÃºt):**

   - Query: "a person playing guitar"
   - Giáº£i thÃ­ch: CLIP encode text â†’ vector â†’ cosine similarity
   - Show top 3 results vá»›i scores

2. **Image Search (1 phÃºt):**

   - Upload áº£nh báº¥t ká»³ tá»« dataset
   - Show similar images/videos
   - Highlight: Visual similarity, khÃ´ng cáº§n tags

3. **Cross-Modal (1 phÃºt):**

   - Text â†’ Video search
   - Nháº¥n máº¡nh: Unified embedding space

4. **Performance Test (1 phÃºt):**

   - Run benchmark
   - Show live: latency, QPS
   - So sÃ¡nh vá»›i baseline

5. **Health Check (1 phÃºt):**

   - Show Kibana dashboard
   - Cluster GREEN, 3 nodes, 6 shards
   - Real-time metrics

6. **Code Walkthrough (2 phÃºt - náº¿u cÃ²n thá»i gian):**
   - Show `demo_multimodal_search.py`
   - Explain vector encoding
   - Explain ES query

**Backup plan náº¿u demo fail:**

- Screenshot/video demo sáºµn
- Giáº£i thÃ­ch báº±ng slides
- Show code + explain logic

---

## 10. CHECKLIST CHUáº¨N Bá»Š

### âœ… **TrÆ°á»›c buá»•i váº¥n Ä‘Ã¡p:**

**Ká»¹ thuáº­t:**

- [ ] Cluster cháº¡y vÃ  GREEN status
- [ ] 1,010 documents Ä‘Ã£ indexed
- [ ] Demo script test qua 5 features
- [ ] Kibana dashboard má»Ÿ sáºµn
- [ ] Backup screenshots/videos
- [ ] Code walkthrough prepared

**TÃ i liá»‡u:**

- [ ] Äá»c láº¡i BAO_CAO.md toÃ n bá»™
- [ ] Há»c thuá»™c 10 sá»‘ liá»‡u quan trá»ng
- [ ] Review SLIDE_BAO_CAO.md
- [ ] Äá»c HUONG_DAN_VAN_DAP.md nÃ y

**Kiáº¿n thá»©c:**

- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c CLIP model
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c ES cluster architecture
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c sharding & replication
- [ ] Giáº£i thÃ­ch Ä‘Æ°á»£c vector search HNSW
- [ ] So sÃ¡nh Ä‘Æ°á»£c ES vs Solr vs Faiss

**Thá»±c hÃ nh:**

- [ ] Practice demo 3 láº§n
- [ ] Practice tráº£ lá»i 20 cÃ¢u há»i phá»• biáº¿n
- [ ] Chuáº©n bá»‹ cÃ¢u há»i ngÆ°á»£c cho GV (2-3 cÃ¢u)

---

## ğŸ“Š 10 Sá» LIá»†U QUAN TRá»ŒNG (GHI NHá»š)

1. **Dataset:** 1,010 files, 911.88 MB
2. **Cluster:** 3 nodes (es01, es02, es03)
3. **Sharding:** 3 primary + 3 replica = 6 shards
4. **Latency:** 74ms average
5. **Throughput:** 13.4 QPS
6. **Embedding:** 512-dimensional vectors
7. **Model:** CLIP ViT-B/32, 400M parameters
8. **ES version:** 8.11.1
9. **Performance vs Solr:** ES 10% faster
10. **Real data:** 10 HD videos (738MB) tá»« Pexels

---

## ğŸ¯ CÃ‚U Há»I NGÆ¯á»¢C CHO GIáº¢NG VIÃŠN

**Náº¿u GV há»i "Em cÃ³ cÃ¢u há»i gÃ¬ khÃ´ng?", há»i 1 trong 3 cÃ¢u nÃ y:**

1. **"ThÆ°a tháº§y/cÃ´, trong thá»±c táº¿ doanh nghiá»‡p, liá»‡u há»‡ thá»‘ng nhÆ° em lÃ m cÃ³ cáº§n thÃªm components nÃ o Ä‘á»ƒ production-ready khÃ´ng áº¡?"**

   - Má»¥c Ä‘Ã­ch: Show sá»± quan tÃ¢m Ä‘áº¿n real-world application
   - GV cÃ³ thá»ƒ suggest: Authentication, Rate limiting, CDN, etc.

2. **"Em tháº¥y xu hÆ°á»›ng hiá»‡n nay lÃ  multi-modal models nhÆ° GPT-4 Vision. Liá»‡u CLIP cÃ³ bá»‹ thay tháº¿ khÃ´ng, hay váº«n cÃ³ chá»— Ä‘á»©ng áº¡?"**

   - Má»¥c Ä‘Ã­ch: Show awareness vá» trends
   - Thá»ƒ hiá»‡n critical thinking

3. **"Vá» máº·t Big Data, lÃ m sao em biáº¿t khi nÃ o nÃªn scale vertical (tÄƒng resources/node) vs horizontal (thÃªm nodes) áº¡?"**
   - Má»¥c Ä‘Ã­ch: Deep technical question
   - Show interest trong architecture decisions

---

## ğŸ’¡ TIPS CHUNG

**Khi tráº£ lá»i:**

1. **Structure:** NÃ³i rÃµ 3 Ä‘iá»ƒm, 4 lÃ½ do â†’ dá»… theo dÃµi
2. **Evidence:** LuÃ´n cÃ³ sá»‘ liá»‡u cá»¥ thá»ƒ (74ms, 13.4 QPS)
3. **Honest:** Thá»«a nháº­n limitations, nhÆ°ng cÃ³ giáº£i phÃ¡p
4. **Confident:** NÃ³i cháº­m, rÃµ rÃ ng, eye contact

**Khi khÃ´ng biáº¿t:**

- "Em chÆ°a nghiÃªn cá»©u sÃ¢u váº¥n Ä‘á» nÃ y, nhÆ°ng em nghÄ© hÆ°á»›ng giáº£i quyáº¿t cÃ³ thá»ƒ lÃ ..."
- "ÄÃ¢y lÃ  limitation em Ä‘Ã£ noted trong pháº§n háº¡n cháº¿, em sáº½ tÃ¬m hiá»ƒu thÃªm"
- KHÃ”NG bá»‹a, KHÃ”NG guess

**Body language:**

- Äá»©ng tháº³ng, confident
- Hand gestures khi giáº£i thÃ­ch technical
- Smile khi demo thÃ nh cÃ´ng
- Calm khi gáº·p technical issues

---

## ğŸš€ Káº¾T LUáº¬N

**Em Ä‘Ã£:**
âœ… Build complete distributed search system
âœ… Integrate AI (CLIP) vá»›i Big Data (Elasticsearch)
âœ… Demonstrate scalability vá»›i 3-node cluster
âœ… Achieve good performance (74ms, 13.4 QPS)
âœ… Compare vá»›i alternatives (Solr)
âœ… Document thoroughly (2,077 lines report)

**Äiá»ƒm máº¡nh:**

- Real implementation, khÃ´ng pháº£i tutorial copy
- Production-like architecture (distributed, HA)
- Clear metrics vÃ  benchmarks
- Reproducible (Docker, documented)

**Äiá»ƒm yáº¿u (honest):**

- Dataset nhá» (900MB), cÃ³ thá»ƒ lá»›n hÆ¡n
- CPU only, chÆ°a GPU optimization
- Synthetic data nhiá»u, real data Ã­t

**NhÆ°ng demonstrate Ä‘Æ°á»£c:**

- Distributed system concepts
- AI model integration
- Performance optimization
- Scalability planning

---

**CHÃšC Báº N Váº¤N ÄÃP THÃ€NH CÃ”NG! ğŸ“**

---

_File nÃ y tá»•ng há»£p toÃ n bá»™ kiáº¿n thá»©c cáº§n thiáº¿t. Äá»c ká»¹, practice demo, vÃ  tá»± tin tráº£ lá»i!_
